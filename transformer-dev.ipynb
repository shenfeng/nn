{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "torch.set_printoptions(sci_mode=False, precision=4)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def karpathy(c_attn, x, n_head, n_embd, flash=True):\n",
    "    B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "    # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "    q, k, v  = c_attn(x).split(n_embd, dim=2)\n",
    "    k = k.view(B, T, n_head, C // n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "    q = q.view(B, T, n_head, C // n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "    v = v.view(B, T, n_head, C // n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "    bias = torch.tril(torch.ones(T, T)).view(1, 1, T, T)        \n",
    "\n",
    "    # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "    # efficient attention using Flash Attention CUDA kernels\n",
    "    if flash:\n",
    "        y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True)    \n",
    "    else:\n",
    "        # manual implementation of attention\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(bias == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)        \n",
    "    y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 8\n",
    "n_head = 2\n",
    "flash = True\n",
    "device = 'cuda' if flash else 'cpu'\n",
    "torch.manual_seed(13)\n",
    "c_attn = nn.Linear(n_embd, 3 * n_embd, bias=False).to(device)\n",
    "flash = True\n",
    "x = torch.randn(2, 4, n_embd).to(device)\n",
    "karpathy(c_attn, x, n_head, n_embd, flash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[    -0.1038,     -0.1869,     -0.9511,     -0.4516,      0.7749,\n",
       "               -0.8839,     -0.5570,     -0.7959],\n",
       "          [    -0.1598,     -0.5558,     -0.0879,      0.0378,     -0.9518,\n",
       "                0.7826,     -0.0118,     -0.2513],\n",
       "          [    -0.1021,     -0.4652,     -0.1973,      0.0524,     -0.0011,\n",
       "               -0.0733,     -0.2401,     -0.3144],\n",
       "          [     0.1038,     -0.3479,     -0.1774,     -0.1089,     -0.4437,\n",
       "                0.1967,     -0.0915,     -0.2301]],\n",
       " \n",
       "         [[    -0.5489,     -0.8790,      1.0406,      0.5399,      1.1409,\n",
       "               -0.1098,      0.6044,      1.5876],\n",
       "          [    -0.5449,     -0.5713,      0.9675,      0.5635,     -0.0129,\n",
       "                0.4037,      0.4179,      0.9543],\n",
       "          [    -0.2615,     -0.1719,      0.4718,      0.2990,     -0.2377,\n",
       "                0.4195,      0.1971,      0.4427],\n",
       "          [    -0.4090,      0.0866,      0.3747,      0.1671,     -0.4818,\n",
       "               -0.0093,     -0.0275,      0.4524]]], grad_fn=<ViewBackward0>),\n",
       " tensor([[[    -0.1038,     -0.1869,     -0.9511,     -0.4516,      0.7749,\n",
       "               -0.8839,     -0.5570,     -0.7959],\n",
       "          [    -0.1598,     -0.5558,     -0.0879,      0.0378,     -0.9518,\n",
       "                0.7826,     -0.0118,     -0.2513],\n",
       "          [    -0.1021,     -0.4652,     -0.1973,      0.0524,     -0.0011,\n",
       "               -0.0733,     -0.2401,     -0.3144],\n",
       "          [     0.1038,     -0.3479,     -0.1774,     -0.1089,     -0.4437,\n",
       "                0.1967,     -0.0915,     -0.2301]],\n",
       " \n",
       "         [[    -0.5489,     -0.8790,      1.0406,      0.5399,      1.1409,\n",
       "               -0.1098,      0.6044,      1.5876],\n",
       "          [    -0.5449,     -0.5713,      0.9675,      0.5635,     -0.0129,\n",
       "                0.4037,      0.4179,      0.9543],\n",
       "          [    -0.2615,     -0.1719,      0.4718,      0.2990,     -0.2377,\n",
       "                0.4195,      0.1971,      0.4427],\n",
       "          [    -0.4090,      0.0866,      0.3747,      0.1671,     -0.4818,\n",
       "               -0.0093,     -0.0275,      0.4524]]], grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
